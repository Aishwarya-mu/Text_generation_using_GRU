{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8lvEzRg6Gfmh"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ROOT = \"/content/drive/MyDrive/Colab Notebooks\"\n",
    "os.chdir(ROOT)\n",
    "\n",
    "assert os.getcwd() == ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MQyRiqRFG4nk"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "PXqLZDbwHP4m",
    "outputId": "9c1d5cec-b577-4dcb-f1c7-150d4327ca67"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"shakespeare.txt\", \"rb\") as f:\n",
    "    text = f.read().decode(encoding=\"utf-8\")\n",
    "\n",
    "text[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u2B2PYINHiiS",
    "outputId": "fac5d452-b4ec-44bb-b019-27a3059cf95f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "j9lhUwW_Hqer"
   },
   "outputs": [],
   "source": [
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uBEbWEa_H4Xc",
    "outputId": "43939c0f-5272-452a-9c30-0287c1e5cccb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R_8xmG7eIADe",
    "outputId": "f58322ea-e833-49e8-c342-c1bb748c4295"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n"
     ]
    }
   ],
   "source": [
    "char2idx = {uniqChar: idx for idx, uniqChar in enumerate(vocab)}\n",
    "print(char2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ppWvDCP0IW6m",
    "outputId": "5dfef65f-4a90-413b-a645-1451c29190f4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm',\n",
       "       'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'],\n",
       "      dtype='<U1')"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char = np.array(vocab)\n",
    "idx2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SHaMjghfIejI",
    "outputId": "c312b6bc-7312-42d8-b3a0-98539181b7ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 56, ..., 45,  8,  0])"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[char] for char in text])\n",
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "sN6AoceNI3Dz",
    "outputId": "1407978a-b1e2-4562-cf36-8bea6b35cf39"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'First Citizen'"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RFxGN0ZpI8ez",
    "outputId": "72b553d1-66df-4a81-9db1-b77f72b5ddd6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52])"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int[:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yQrFFxj9I_Il",
    "outputId": "00f08e6b-82d1-40ae-b9a5-58234643a935"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11043"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length + 1)\n",
    "examples_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRXdFE0HJMXh"
   },
   "outputs": [],
   "source": [
    "# text --> \"Hello\" ---> \"Hell\" --> \"ello\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iqAtPk0aJZ2J",
    "outputId": "208a73f2-908d-46b9-cacd-826830108f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n"
     ]
    }
   ],
   "source": [
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2zeWKcDaJ6fo",
    "outputId": "0475a77e-ae75-458b-ecfc-89ddff1ec9bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "    print(repr(\"\".join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUQjKLWMKdQ5",
    "outputId": "e7f90aec-51a3-4aca-ce5a-47f81f70f848"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'te \\n xt'\n"
     ]
    }
   ],
   "source": [
    "print(repr(\"te \\n xt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QSxfx2jBKgUQ"
   },
   "outputs": [],
   "source": [
    "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euoDRGeULCCI"
   },
   "outputs": [],
   "source": [
    "'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8RUWZ4g8LNFD"
   },
   "outputs": [],
   "source": [
    "def split_input_taget(chunk):\n",
    "    # chunk --> First\n",
    "    input_text = chunk[:-1] # Firs \n",
    "    target_text = chunk[1:] # irst\n",
    "\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "6ed6-0I7MVK_"
   },
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_taget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "azpTktfMMd5B",
    "outputId": "e95ba72e-0fe4-4170-f97e-4be288029e5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input_data:-\n",
      " 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "target_data:-\n",
      " 'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input_data:-\\n\", repr(\"\".join(idx2char[input_example.numpy()])))\n",
    "    print(\"target_data:-\\n\", repr(\"\".join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-D2OUh19NAD9"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    BATCH_SIZE = 64\n",
    "    BUFFER_SIZE = 10000\n",
    "\n",
    "    # Length of the vocabulary in chars\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # The embedding dimension\n",
    "    embedding_dim = 256\n",
    "\n",
    "    # Number of RNN units\n",
    "    rnn_units = 1024    \n",
    "\n",
    "    # Directory where the checkpoints will be saved\n",
    "    checkpoint_dir = 'training_checkpoints'\n",
    "\n",
    "    EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "R534eHOKNXhC"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(Config.BUFFER_SIZE).batch(Config.BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "_Mg9c1iXNrBE"
   },
   "outputs": [],
   "source": [
    "def get_model(BATCH_SIZE=Config.BATCH_SIZE):\n",
    "    embedding_layer = tf.keras.layers.Embedding(Config.vocab_size, Config.embedding_dim, batch_input_shape=[BATCH_SIZE, None])\n",
    "\n",
    "    layers = [\n",
    "              embedding_layer,\n",
    "              tf.keras.layers.GRU(Config.rnn_units, \n",
    "                                  return_sequences=True, \n",
    "                                  stateful=True, \n",
    "                                  recurrent_initializer='glorot_uniform'),\n",
    "              tf.keras.layers.Dense(Config.vocab_size) # logits\n",
    "    ]\n",
    "\n",
    "    return tf.keras.Sequential(layers=layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Vn2AxVyXPk6J"
   },
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZJpF9ixPsMr",
    "outputId": "b05a5c30-9a6f-41a8-b89f-227c095437c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "j8qbYJwdPuUT"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "Xt_R-PrIQYrF"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "8yOPPKB_Qg08"
   },
   "outputs": [],
   "source": [
    "checkpoint_prefix = os.path.join(Config.checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RjFRPtRwRI6z",
    "outputId": "77999590-fa47-4aae-c128-bbe4c3d23c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 17s 57ms/step - loss: 3.3055\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 11s 57ms/step - loss: 2.0448\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 11s 58ms/step - loss: 1.7369\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 11s 59ms/step - loss: 1.5652\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 11s 60ms/step - loss: 1.4632\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 11s 58ms/step - loss: 1.3966\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 11s 58ms/step - loss: 1.3468\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 11s 58ms/step - loss: 1.3058\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 11s 58ms/step - loss: 1.2690\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 11s 58ms/step - loss: 1.2356\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=Config.EPOCHS, callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Kkd5LUr9RVx2",
    "outputId": "71414f04-0ae5-4356-a56f-c8afc94f5381"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'training_checkpoints/ckpt_10'"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(Config.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FXBu_ZFIR8-j"
   },
   "outputs": [],
   "source": [
    "INFERENCE_BATCH_SIZE = 1\n",
    "model2 = get_model(BATCH_SIZE=INFERENCE_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLlr99kqSUGu",
    "outputId": "5cc78999-808f-42e0-f7ac-e64566114e9c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f2c61f78ef0>"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.load_weights(tf.train.latest_checkpoint(Config.checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "96nJk2LqSgy_"
   },
   "outputs": [],
   "source": [
    "model2.build(tf.TensorShape([INFERENCE_BATCH_SIZE, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PkdWMIa5SpbE",
    "outputId": "12824e14-1e1f-4fd9-d481-7eb2cd5112d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            16640     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 65)             66625     \n",
      "=================================================================\n",
      "Total params: 4,021,569\n",
      "Trainable params: 4,021,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Ki3E0WOOSryP"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "\n",
    "    num_gen = 1000\n",
    "\n",
    "    input_str = [char2idx[s] for s in start_string]\n",
    "    input_str = tf.expand_dims(input_str, 0)\n",
    "\n",
    "    text_generated = list()\n",
    "\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_gen):\n",
    "        predictions = model(input_str)\n",
    "\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "\n",
    "        input_str = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return start_string + \"\".join(text_generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AUY-wR8rTxIP",
    "outputId": "90ee1586-5d7b-46f8-bf58-ab7076866545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: Camillo, and I know not, she was for 'ever!\n",
      "\n",
      "ANTONIO:\n",
      "Sir, so done, yea to be lad king,\n",
      "Distrusting sous with very possess'd, be pow'd and store your hands\n",
      "Show'd an end of malignal: come, in thy wars two must believe\n",
      "How ond: here let us have sure by,\n",
      "And mortal gentleman, a full filties that was\n",
      "Thy very poison of a man of cut.\n",
      "\n",
      "BAPTISTA:\n",
      "Ay, the people\n",
      "Thy daughter: whose deers before 'Ew with a present, and come about\n",
      "Their people that I would be my tear upon him.\n",
      "\n",
      "First Musician:\n",
      "Ay, hear me, tell Welcees his spreet,\n",
      "Shall open you on what with me as, look will have been still thy son!\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "You are pact good one: Camillo.\n",
      "\n",
      "CURTES:\n",
      "Is he lest kill those rope-facter's death?\n",
      "Que, hear her godden slave is,\n",
      "With strangers hulf way with him,\n",
      "Having deeply frowh I dured paulard's unclew throne.\n",
      "\n",
      "PISTRES:\n",
      "Ay, to the prince of this face tremble him to be thought not call her behind,\n",
      "And thoughts all unprovance us from man's part,\n",
      "If not chequest?\n",
      "\n",
      "ARIEL:\n",
      "Sir, who knows not; an\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model2, start_string=\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flbKvpSbWNuJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Text_gen_demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
